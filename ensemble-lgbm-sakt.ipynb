{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-31T21:49:17.400056Z",
     "iopub.status.busy": "2020-12-31T21:49:17.342030Z",
     "iopub.status.idle": "2020-12-31T21:49:17.403044Z",
     "shell.execute_reply": "2020-12-31T21:49:17.402398Z"
    },
    "papermill": {
     "duration": 0.139134,
     "end_time": "2020-12-31T21:49:17.403156",
     "exception": false,
     "start_time": "2020-12-31T21:49:17.264022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T21:49:17.432599Z",
     "iopub.status.busy": "2020-12-31T21:49:17.431890Z",
     "iopub.status.idle": "2020-12-31T21:49:18.570083Z",
     "shell.execute_reply": "2020-12-31T21:49:18.570730Z"
    },
    "papermill": {
     "duration": 1.155495,
     "end_time": "2020-12-31T21:49:18.570894",
     "exception": false,
     "start_time": "2020-12-31T21:49:17.415399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       ".datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import riiideducation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T21:49:18.601680Z",
     "iopub.status.busy": "2020-12-31T21:49:18.600925Z",
     "iopub.status.idle": "2020-12-31T21:52:30.679850Z",
     "shell.execute_reply": "2020-12-31T21:52:30.680733Z"
    },
    "papermill": {
     "duration": 192.096397,
     "end_time": "2020-12-31T21:52:30.680970",
     "exception": false,
     "start_time": "2020-12-31T21:49:18.584573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 10 s, total: 2min 26s\n",
      "Wall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import psutil\n",
    "####\n",
    "MAX_SEQ = 100\n",
    "####\n",
    "\n",
    "\n",
    "dtype = {'timestamp':'int64', \n",
    "         'user_id':'int32' ,\n",
    "         'content_id':'int16',\n",
    "         'content_type_id':'int8',\n",
    "         'answered_correctly':'int8'}\n",
    "\n",
    "train_df = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/train.csv', usecols=[1, 2, 3, 4, 7], dtype=dtype)\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "train_df = train_df[train_df.content_type_id == False]\n",
    "\n",
    "#arrange by timestamp\n",
    "train_df = train_df.sort_values(['timestamp'], ascending=True).reset_index(drop = True)\n",
    "\n",
    "\n",
    "skills = train_df[\"content_id\"].unique()\n",
    "n_skill = len(skills)\n",
    "\n",
    "group = train_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (\n",
    "            r['content_id'].values,\n",
    "            r['answered_correctly'].values))\n",
    "\n",
    "del train_df\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "class SAKTDataset(Dataset):\n",
    "    def __init__(self, group, n_skill, max_seq=MAX_SEQ): ####### 100\n",
    "        super(SAKTDataset, self).__init__()\n",
    "        self.max_seq = max_seq\n",
    "        self.n_skill = n_skill\n",
    "        self.samples = group\n",
    "        \n",
    "        self.user_ids = []\n",
    "        for user_id in group.index:\n",
    "            q, qa = group[user_id]\n",
    "            if len(q) < 2: ####### 10\n",
    "                continue\n",
    "            self.user_ids.append(user_id)\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user_id = self.user_ids[index]\n",
    "        q_, qa_ = self.samples[user_id]\n",
    "        seq_len = len(q_)\n",
    "\n",
    "        q = np.zeros(self.max_seq, dtype=int)\n",
    "        qa = np.zeros(self.max_seq, dtype=int)\n",
    "        \n",
    "        if seq_len >= self.max_seq:\n",
    "            \n",
    "            if random.random()>0.1:\n",
    "                start = random.randint(0,(seq_len-self.max_seq))\n",
    "                end = start + self.max_seq\n",
    "                q[:] = q_[start:end]\n",
    "                qa[:] = qa_[start:end]\n",
    "            else:\n",
    "                \n",
    "                q[:] = q_[-self.max_seq:]\n",
    "                qa[:] = qa_[-self.max_seq:]\n",
    "        else:\n",
    "            \n",
    "            if random.random()>0.1:\n",
    "                \n",
    "                start = 0\n",
    "                end = random.randint(2,seq_len)\n",
    "                seq_len = end - start\n",
    "                q[-seq_len:] = q_[0:seq_len]\n",
    "                qa[-seq_len:] = qa_[0:seq_len]\n",
    "            else:\n",
    "                \n",
    "                q[-seq_len:] = q_\n",
    "                qa[-seq_len:] = qa_\n",
    "\n",
    "        \n",
    "        target_id = q[1:]\n",
    "        label = qa[1:]\n",
    "\n",
    "        x = np.zeros(self.max_seq-1, dtype=int)\n",
    "        x = q[:-1].copy()\n",
    "        x += (qa[:-1] == 1) * self.n_skill\n",
    "\n",
    "        return x, target_id, label\n",
    "    \n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, state_size=200):\n",
    "        super(FFN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.lr1 = nn.Linear(state_size, state_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lr2 = nn.Linear(state_size, state_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.lr1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lr2(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def future_mask(seq_length):\n",
    "    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n",
    "    return torch.from_numpy(future_mask)\n",
    "\n",
    "\n",
    "class SAKTModel(nn.Module):\n",
    "    def __init__(self, n_skill, max_seq=MAX_SEQ, embed_dim=128): ####### 100->MAX_SEQ\n",
    "        super(SAKTModel, self).__init__()\n",
    "        self.n_skill = n_skill\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(2*n_skill+1, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_seq-1, embed_dim)\n",
    "        self.e_embedding = nn.Embedding(n_skill+1, embed_dim)\n",
    "\n",
    "        self.multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=0.2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.layer_normal = nn.LayerNorm(embed_dim) \n",
    "\n",
    "        self.ffn = FFN(embed_dim)\n",
    "        self.pred = nn.Linear(embed_dim, 1)\n",
    "    \n",
    "    def forward(self, x, question_ids):\n",
    "        device = x.device        \n",
    "        x = self.embedding(x)\n",
    "        pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)\n",
    "\n",
    "        pos_x = self.pos_embedding(pos_id)\n",
    "        x = x + pos_x\n",
    "\n",
    "        e = self.e_embedding(question_ids)\n",
    "\n",
    "        x = x.permute(1, 0, 2) # x: [bs, s_len, embed] => [s_len, bs, embed]\n",
    "        e = e.permute(1, 0, 2)\n",
    "        att_mask = future_mask(x.size(0)).to(device)\n",
    "        att_output, att_weight = self.multi_att(e, x, x, attn_mask=att_mask)\n",
    "        att_output = self.layer_normal(att_output + e)\n",
    "        att_output = att_output.permute(1, 0, 2) # att_output: [s_len, bs, embed] => [bs, s_len, embed]\n",
    "\n",
    "        x = self.ffn(att_output)\n",
    "        x = self.layer_normal(x + att_output)\n",
    "        x = self.pred(x)\n",
    "\n",
    "        return x.squeeze(-1), att_weight\n",
    "    \n",
    "\n",
    "\n",
    "def train_epoch(model, train_iterator, optim, criterion, device=\"cuda\"):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = []\n",
    "    num_corrects = 0\n",
    "    num_total = 0\n",
    "    labels = []\n",
    "    outs = []\n",
    "\n",
    "    tbar = tqdm(train_iterator)\n",
    "    for item in tbar:\n",
    "        x = item[0].to(device).long()\n",
    "        target_id = item[1].to(device).long()\n",
    "        label = item[2].to(device).float()\n",
    "\n",
    "        optim.zero_grad()\n",
    "        output, atten_weight = model(x, target_id)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        output = output[:, -1]\n",
    "        label = label[:, -1] \n",
    "        pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "        \n",
    "        num_corrects += (pred == label).sum().item()\n",
    "        num_total += len(label)\n",
    "\n",
    "        labels.extend(label.view(-1).data.cpu().numpy())\n",
    "        outs.extend(output.view(-1).data.cpu().numpy())\n",
    "\n",
    "        tbar.set_description('loss - {:.4f}'.format(loss))\n",
    "\n",
    "    acc = num_corrects / num_total\n",
    "    auc = roc_auc_score(labels, outs)\n",
    "    loss = np.mean(train_loss)\n",
    "\n",
    "    return loss, acc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T21:52:30.795588Z",
     "iopub.status.busy": "2020-12-31T21:52:30.794518Z",
     "iopub.status.idle": "2020-12-31T21:52:30.800781Z",
     "shell.execute_reply": "2020-12-31T21:52:30.800121Z"
    },
    "papermill": {
     "duration": 0.106474,
     "end_time": "2020-12-31T21:52:30.800918",
     "exception": false,
     "start_time": "2020-12-31T21:52:30.694444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "#del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T21:52:30.849034Z",
     "iopub.status.busy": "2020-12-31T21:52:30.847836Z",
     "iopub.status.idle": "2020-12-31T21:52:40.926911Z",
     "shell.execute_reply": "2020-12-31T21:52:40.926274Z"
    },
    "papermill": {
     "duration": 10.112367,
     "end_time": "2020-12-31T21:52:40.927042",
     "exception": false,
     "start_time": "2020-12-31T21:52:30.814675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, samples, test_df, skills, max_seq=MAX_SEQ): ####### 100\n",
    "        super(TestDataset, self).__init__()\n",
    "        self.samples = samples\n",
    "        self.user_ids = [x for x in test_df[\"user_id\"].unique()]\n",
    "        self.test_df = test_df\n",
    "        self.skills = skills\n",
    "        self.n_skill = len(skills)\n",
    "        self.max_seq = max_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.test_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        test_info = self.test_df.iloc[index]\n",
    "\n",
    "        user_id = test_info[\"user_id\"]\n",
    "        target_id = test_info[\"content_id\"]\n",
    "\n",
    "        q = np.zeros(self.max_seq, dtype=int)\n",
    "        qa = np.zeros(self.max_seq, dtype=int)\n",
    "\n",
    "        if user_id in self.samples.index:\n",
    "            q_, qa_ = self.samples[user_id]\n",
    "            \n",
    "            seq_len = len(q_)\n",
    "\n",
    "            if seq_len >= self.max_seq:\n",
    "                q = q_[-self.max_seq:]\n",
    "                qa = qa_[-self.max_seq:]\n",
    "            else:\n",
    "                q[-seq_len:] = q_\n",
    "                qa[-seq_len:] = qa_          \n",
    "        \n",
    "        x = np.zeros(self.max_seq-1, dtype=int)\n",
    "        x = q[1:].copy()\n",
    "        x += (qa[1:] == 1) * self.n_skill\n",
    "        \n",
    "        questions = np.append(q[2:], [target_id])\n",
    "        \n",
    "        return x, questions\n",
    "    \n",
    "for user_id in group.index:\n",
    "    q, qa = group[user_id]\n",
    "    if len(q)>MAX_SEQ:\n",
    "        group[user_id] = (q[-MAX_SEQ:],qa[-MAX_SEQ:])\n",
    "        \n",
    "import pickle\n",
    "pickle.dump(group, open(\"group.pkl\", \"wb\"))\n",
    "del group\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T21:52:40.965275Z",
     "iopub.status.busy": "2020-12-31T21:52:40.964522Z",
     "iopub.status.idle": "2020-12-31T21:52:43.914329Z",
     "shell.execute_reply": "2020-12-31T21:52:43.913775Z"
    },
    "papermill": {
     "duration": 2.973294,
     "end_time": "2020-12-31T21:52:43.914449",
     "exception": false,
     "start_time": "2020-12-31T21:52:40.941155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SAKT_model = SAKTModel(n_skill, embed_dim=128)\n",
    "try:\n",
    "    SAKT_model.load_state_dict(torch.load('../input/riiid-sakt-model-training-with-gpu-and-inference/SAKT_model.pt'))\n",
    "except:\n",
    "    SAKT_model.load_state_dict(torch.load('../input/riiid-sakt-model-training-with-gpu-and-inference/SAKT_model.pt', map_location='cpu'))\n",
    "\n",
    "SAKT_model.to(device)\n",
    "SAKT_model.eval()\n",
    "\n",
    "import pickle\n",
    "group = pickle.load(open(\"group.pkl\", \"rb\"))\n",
    "\n",
    "print(psutil.virtual_memory().percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T21:52:43.966834Z",
     "iopub.status.busy": "2020-12-31T21:52:43.964994Z",
     "iopub.status.idle": "2020-12-31T21:52:44.002019Z",
     "shell.execute_reply": "2020-12-31T21:52:44.002564Z"
    },
    "papermill": {
     "duration": 0.073781,
     "end_time": "2020-12-31T21:52:44.002707",
     "exception": false,
     "start_time": "2020-12-31T21:52:43.928926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def añadir_features(df, features_dicts):\n",
    "    \n",
    "    #Usuario\n",
    "    tasa_acierto_usuario = np.zeros(len(df), dtype = np.float32)\n",
    "    elapsed_time_u_avg = np.zeros(len(df), dtype = np.float32)\n",
    "    explanation_u_avg = np.zeros(len(df), dtype = np.float32)\n",
    "    user_pause_timestamp_1 = np.zeros(len(df), dtype = np.float32)\n",
    "    user_pause_timestamp_2 = np.zeros(len(df), dtype = np.float32)\n",
    "    user_pause_timestamp_3 = np.zeros(len(df), dtype = np.float32)\n",
    "    user_pause_timestamp_incorrect = np.zeros(len(df), dtype = np.float32)\n",
    "    cont_preguntas_corr_user_f = np.zeros(len(df), dtype = np.int32)\n",
    "    cont_preguntas_user_f = np.zeros(len(df), dtype = np.int32)\n",
    "    CUMULATIVE_ELO_USER = np.zeros(len(df), dtype = np.int32)\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Question features\n",
    "    tasa_acierto_pregunta = np.zeros(len(df), dtype = np.float32)\n",
    "    elapsed_time_q_avg = np.zeros(len(df), dtype = np.float32)\n",
    "    explanation_q_avg = np.zeros(len(df), dtype = np.float32)\n",
    "    # -----------------------------------------------------------------------\n",
    "    # User Question\n",
    "    intentos = np.zeros(len(df), dtype = np.int8)\n",
    "    \n",
    "    for num, row in enumerate(tqdm(df[['user_id', 'content_id', 'prior_question_elapsed_time', 'prior_question_had_explanation', 'timestamp','mean_question_accuracy']].itertuples(), total=df.shape[0])):\n",
    "        \n",
    "        #ELO\n",
    "        CUMULATIVE_ELO_USER[num] = features_dicts['CUMULATIVE_ELO_USER'][row.user_id]\n",
    "        \n",
    "        # User features\n",
    "        # ------------------------------------------------------------------\n",
    "        if features_dicts['cont_preguntas_user'][row.user_id] != 0:\n",
    "            tasa_acierto_usuario[num] = features_dicts['cont_preguntas_corr_user'][row.user_id] / features_dicts['cont_preguntas_user'][row.user_id]\n",
    "            elapsed_time_u_avg[num] = features_dicts['elapsed_time_u_sum'][row.user_id] / features_dicts['cont_preguntas_user'][row.user_id]\n",
    "            explanation_u_avg[num] = features_dicts['explanation_u_sum'][row.user_id] / features_dicts['cont_preguntas_user'][row.user_id]\n",
    "            cont_preguntas_corr_user_f[num] = features_dicts['cont_preguntas_corr_user'][row.user_id]\n",
    "            cont_preguntas_user_f[num] = features_dicts['cont_preguntas_user'][row.user_id]\n",
    "            \n",
    "        else:\n",
    "            tasa_acierto_usuario[num] = np.nan\n",
    "            elapsed_time_u_avg[num] = np.nan\n",
    "            explanation_u_avg[num] = np.nan\n",
    "            cont_preguntas_corr_user_f[num] = 0\n",
    "            cont_preguntas_user_f[num] = 0\n",
    "\n",
    "            \n",
    "        if len(features_dicts['timestamp_u'][row.user_id]) == 0:\n",
    "            user_pause_timestamp_1[num] = np.nan\n",
    "            user_pause_timestamp_2[num] = np.nan\n",
    "            user_pause_timestamp_3[num] = np.nan\n",
    "        elif len(features_dicts['timestamp_u'][row.user_id]) == 1:\n",
    "            user_pause_timestamp_1[num] = row.timestamp - features_dicts['timestamp_u'][row.user_id][0]\n",
    "            user_pause_timestamp_2[num] = np.nan\n",
    "            user_pause_timestamp_3[num] = np.nan\n",
    "        elif len(features_dicts['timestamp_u'][row.user_id]) == 2:\n",
    "            user_pause_timestamp_1[num] = row.timestamp - features_dicts['timestamp_u'][row.user_id][1]\n",
    "            user_pause_timestamp_2[num] = row.timestamp - features_dicts['timestamp_u'][row.user_id][0]\n",
    "            user_pause_timestamp_3[num] = np.nan\n",
    "        elif len(features_dicts['timestamp_u'][row.user_id]) == 3:\n",
    "            user_pause_timestamp_1[num] = row.timestamp - features_dicts['timestamp_u'][row.user_id][2]\n",
    "            user_pause_timestamp_2[num] = row.timestamp - features_dicts['timestamp_u'][row.user_id][1]\n",
    "            user_pause_timestamp_3[num] = row.timestamp - features_dicts['timestamp_u'][row.user_id][0]\n",
    "        \n",
    "        user_pause_timestamp_incorrect[num] = row.timestamp - features_dicts['timestamp_u_incorrect'][row.user_id]\n",
    "          \n",
    "        # ------------------------------------------------------------------\n",
    "        # Question features assignation\n",
    "        if features_dicts['cont_preguntas'][row.content_id] != 0:\n",
    "            tasa_acierto_pregunta[num] = features_dicts['cont_preguntas_corr'][row.content_id] / features_dicts['cont_preguntas'][row.content_id]\n",
    "            elapsed_time_q_avg[num] = features_dicts['elapsed_time_q_sum'][row.content_id] / features_dicts['cont_preguntas'][row.content_id]\n",
    "            explanation_q_avg[num] = features_dicts['explanation_q_sum'][row.content_id] / features_dicts['cont_preguntas'][row.content_id]\n",
    "        else:\n",
    "            tasa_acierto_pregunta[num] = np.nan\n",
    "            elapsed_time_q_avg[num] = np.nan\n",
    "            explanation_q_avg[num] = np.nan\n",
    "        # ------------------------------------------------------------------\n",
    "        # User Question assignation\n",
    "        intentos[num] = features_dicts['intentos_dict'][row.user_id][row.content_id]\n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "        # ------------------------------------------------------------------\n",
    "        # Actualizaciones\n",
    "        features_dicts['cont_preguntas_user'][row.user_id] += 1\n",
    "        features_dicts['elapsed_time_u_sum'][row.user_id] += row.prior_question_elapsed_time\n",
    "        features_dicts['explanation_u_sum'][row.user_id] += row.prior_question_had_explanation\n",
    "        if len(features_dicts['timestamp_u'][row.user_id]) == 3:\n",
    "            features_dicts['timestamp_u'][row.user_id].pop(0)\n",
    "            features_dicts['timestamp_u'][row.user_id].append(row.timestamp)\n",
    "        else:\n",
    "            features_dicts['timestamp_u'][row.user_id].append(row.timestamp)\n",
    "        # ------------------------------------------------------------------\n",
    "        # Question features updates\n",
    "        features_dicts['cont_preguntas'][row.content_id] += 1\n",
    "        features_dicts['elapsed_time_q_sum'][row.content_id] += row.prior_question_elapsed_time\n",
    "        features_dicts['explanation_q_sum'][row.content_id] += row.prior_question_had_explanation\n",
    "        # ------------------------------------------------------------------\n",
    "        #User Question updates\n",
    "        features_dicts['intentos_dict'][row.user_id][row.content_id] += 1\n",
    "        # ------------------------------------------------------------------\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        #Actualizacion ELO\n",
    "        features_dicts['CUMULATIVE_ELO_USER'][row.user_id] += (1 - row.mean_question_accuracy)*100\n",
    "    \n",
    "    user_df = pd.DataFrame({'%_acierto_usuario': tasa_acierto_usuario, 'elapsed_time_u_avg': elapsed_time_u_avg, 'explanation_u_avg': explanation_u_avg, \n",
    "                            '%_acierto_pregunta_CONT': tasa_acierto_pregunta, 'elapsed_time_q_avg': elapsed_time_q_avg, 'explanation_q_avg': explanation_q_avg, \n",
    "                            'intentos': intentos, 'user_pause_timestamp_1': user_pause_timestamp_1, 'user_pause_timestamp_2': user_pause_timestamp_2,\n",
    "                            'user_pause_timestamp_3': user_pause_timestamp_3, 'user_pause_timestamp_incorrect': user_pause_timestamp_incorrect,\n",
    "                            'cont_preguntas_corr_user':cont_preguntas_corr_user_f, 'cont_preguntas_user': cont_preguntas_user_f, 'CUMULATIVE_ELO_USER':CUMULATIVE_ELO_USER}) \n",
    "    \n",
    "    \n",
    "    del tasa_acierto_usuario, cont_preguntas_user_f,CUMULATIVE_ELO_USER, cont_preguntas_corr_user_f, elapsed_time_u_avg, explanation_u_avg, tasa_acierto_pregunta, elapsed_time_q_avg, explanation_q_avg, intentos, user_pause_timestamp_1, user_pause_timestamp_2,user_pause_timestamp_3, user_pause_timestamp_incorrect\n",
    " \n",
    "    df = pd.concat([df, user_df], axis = 1)\n",
    "    del user_df\n",
    "    \n",
    "    #Features extra\n",
    "    df['correction'] = df['user_pause_timestamp_1'] / df['user_pause_timestamp_incorrect'] + df['prior_question_had_explanation'] + df['intentos']\n",
    "    df['user_pause_timestamp_ratio_1'] = df['user_pause_timestamp_1'] / df['user_pause_timestamp_2']\n",
    "    df['%_media_armonica'] = 2*df['%_acierto_usuario']*df['mean_question_accuracy']/(df['%_acierto_usuario'] + df['mean_question_accuracy'])\n",
    "    df['%_media_armonica'].fillna(0.642673913, inplace = True)\n",
    "    df['user_pause_timestamp_MEAN'] = (df['user_pause_timestamp_1'] + df['user_pause_timestamp_2'] + df['user_pause_timestamp_3'])/3\n",
    "    df['user_pause_timestamp_MEAN_RATIO'] = df['user_pause_timestamp_1']/df['user_pause_timestamp_MEAN']\n",
    "    df['ELO'] = (df['CUMULATIVE_ELO_USER'] + 4*df['user_pause_timestamp_MEAN_RATIO']*df['cont_preguntas_corr_user'] - 4*df['user_pause_timestamp_MEAN_RATIO']*(df['cont_preguntas_user']-df['cont_preguntas_corr_user']))/df['cont_preguntas_user']\n",
    "    df.replace(np.inf, 0, inplace = True)\n",
    "    df[['ELO','correction','user_pause_timestamp_ratio_1','user_pause_timestamp_MEAN','%_media_armonica','user_pause_timestamp_MEAN_RATIO']] = df[['ELO','correction','user_pause_timestamp_ratio_1','user_pause_timestamp_MEAN','%_media_armonica','user_pause_timestamp_MEAN_RATIO']].astype(np.float32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T21:52:44.035155Z",
     "iopub.status.busy": "2020-12-31T21:52:44.034455Z",
     "iopub.status.idle": "2020-12-31T21:52:44.048503Z",
     "shell.execute_reply": "2020-12-31T21:52:44.049060Z"
    },
    "papermill": {
     "duration": 0.031979,
     "end_time": "2020-12-31T21:52:44.049210",
     "exception": false,
     "start_time": "2020-12-31T21:52:44.017231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def actualizar_features_inicio(df, features_dicts):\n",
    "    \n",
    "    for num, row in enumerate(tqdm(df[['user_id', 'answered_correctly', 'content_id', 'prior_question_elapsed_time', 'prior_question_had_explanation', 'timestamp','mean_question_accuracy']].itertuples(), total=df.shape[0])):\n",
    "        features_dicts['cont_preguntas_user'][row.user_id] += 1\n",
    "        features_dicts['elapsed_time_u_sum'][row.user_id] += row.prior_question_elapsed_time\n",
    "        features_dicts['explanation_u_sum'][row.user_id] += row.prior_question_had_explanation\n",
    "        if len(features_dicts['timestamp_u'][row.user_id]) == 3:\n",
    "            features_dicts['timestamp_u'][row.user_id].pop(0)\n",
    "            features_dicts['timestamp_u'][row.user_id].append(row.timestamp)\n",
    "        else:\n",
    "            features_dicts['timestamp_u'][row.user_id].append(row.timestamp)\n",
    "        # ------------------------------------------------------------------\n",
    "        # Question features updates\n",
    "        features_dicts['cont_preguntas'][row.content_id] += 1\n",
    "        features_dicts['elapsed_time_q_sum'][row.content_id] += row.prior_question_elapsed_time\n",
    "        features_dicts['explanation_q_sum'][row.content_id] += row.prior_question_had_explanation\n",
    "        # ------------------------------------------------------------------\n",
    "        # User Question updates\n",
    "        features_dicts['intentos_dict'][row.user_id][row.content_id] += 1\n",
    "        # ------------------------------------------------------------------\n",
    "        # ------------------------------------------------------------------\n",
    "        # User features updates\n",
    "        features_dicts['cont_preguntas_corr_user'][row.user_id] += row.answered_correctly\n",
    "        if row.answered_correctly == 0:\n",
    "            features_dicts['timestamp_u_incorrect'][row.user_id] = row.timestamp\n",
    "        # ------------------------------------------------------------------\n",
    "        # Question features updates\n",
    "        features_dicts['cont_preguntas_corr'][row.content_id] += row.answered_correctly\n",
    "        #actualizacion ELO\n",
    "        features_dicts['CUMULATIVE_ELO_USER'][row.user_id] += (1 - row.mean_question_accuracy)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T21:52:44.082003Z",
     "iopub.status.busy": "2020-12-31T21:52:44.081335Z",
     "iopub.status.idle": "2020-12-31T21:52:44.088222Z",
     "shell.execute_reply": "2020-12-31T21:52:44.088709Z"
    },
    "papermill": {
     "duration": 0.024843,
     "end_time": "2020-12-31T21:52:44.088844",
     "exception": false,
     "start_time": "2020-12-31T21:52:44.064001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def actualizar_features(df, features_dicts):\n",
    "    \n",
    "    for row in tqdm(df[['user_id', 'answered_correctly', 'content_id', 'timestamp']].itertuples(), total=df.shape[0]):\n",
    "        # User features updates\n",
    "        features_dicts['cont_preguntas_corr_user'][row.user_id] += row.answered_correctly\n",
    "        if row.answered_correctly == 0:\n",
    "            features_dicts['timestamp_u_incorrect'][row.user_id] = row.timestamp\n",
    "        # ------------------------------------------------------------------\n",
    "        # Question features updates\n",
    "        features_dicts['cont_preguntas_corr'][row.content_id] += row.answered_correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T21:52:44.122133Z",
     "iopub.status.busy": "2020-12-31T21:52:44.121377Z",
     "iopub.status.idle": "2020-12-31T21:52:44.125997Z",
     "shell.execute_reply": "2020-12-31T21:52:44.126518Z"
    },
    "papermill": {
     "duration": 0.022938,
     "end_time": "2020-12-31T21:52:44.126665",
     "exception": false,
     "start_time": "2020-12-31T21:52:44.103727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open('../input/lgbm-model-riiid-training/' + name + '.pkl',mode =  'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T21:52:44.159750Z",
     "iopub.status.busy": "2020-12-31T21:52:44.159115Z",
     "iopub.status.idle": "2020-12-31T22:14:34.723817Z",
     "shell.execute_reply": "2020-12-31T22:14:34.723203Z"
    },
    "papermill": {
     "duration": 1310.582217,
     "end_time": "2020-12-31T22:14:34.723931",
     "exception": false,
     "start_time": "2020-12-31T21:52:44.141714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTUALIZACION DE FEATURES INICIADO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99271300/99271300 [17:14<00:00, 95961.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTUALIZACION DE FEATURES FINALIZADO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ACTUALIZACION DE FEATURES\n",
    "columnas = ['timestamp', 'user_id', 'answered_correctly', 'content_id', 'content_type_id', 'prior_question_elapsed_time', 'prior_question_had_explanation']\n",
    "dtypes={'user_id': 'int32', \n",
    "        'content_id': 'int16',\n",
    "        'task_container_id': 'int16',\n",
    "        'content_type_id': 'int8',\n",
    "        'answered_correctly': 'int8', \n",
    "        'prior_question_elapsed_time': 'float32',\n",
    "        'prior_question_had_explanation': 'boolean',\n",
    "        'timestamp':'int64',}\n",
    "\n",
    "\n",
    "train = pd.read_csv('../input/riiid-test-answer-prediction/train.csv', usecols = columnas, dtype = dtypes)#, nrows = 1000)\n",
    "#train.sort_values(by=['user_id'], inplace = True)\n",
    "\n",
    "#Creamos dicionarios\n",
    "features_dicts = {'cont_preguntas_user': defaultdict(int),\n",
    "                  'cont_preguntas_corr_user': defaultdict(int),\n",
    "                  'elapsed_time_u_sum': defaultdict(int),\n",
    "                  'explanation_u_sum': defaultdict(int),\n",
    "                  'cont_preguntas': defaultdict(int),\n",
    "                  'cont_preguntas_corr': defaultdict(int),\n",
    "                  'elapsed_time_q_sum': defaultdict(int),\n",
    "                  'explanation_q_sum': defaultdict(int),\n",
    "                  'intentos_dict': defaultdict(lambda: defaultdict(int)),\n",
    "                  'timestamp_u': defaultdict(list),\n",
    "                  'timestamp_u_incorrect': defaultdict(int),\n",
    "                  'CUMULATIVE_ELO_USER': defaultdict(int)}\n",
    "\n",
    "#eliminacion de lectures\n",
    "train = train.loc[train.content_type_id == False].reset_index(drop = True)\n",
    "\n",
    "#Limpieza\n",
    "prior_question_elapsed_time_mean = train['prior_question_elapsed_time'].dropna().values.mean()\n",
    "train['prior_question_had_explanation'] = train.prior_question_had_explanation.fillna(False).astype('int8')\n",
    "\n",
    "train['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean, inplace = True)\n",
    "\n",
    "#QUESTIONS MEAN\n",
    "columnas = ['content_id','mean_question_accuracy','std_accuracy']\n",
    "dtypes={'content_id': 'int16', 'mean_question_accuracy': 'float32', 'std_accuracy': 'float32'}\n",
    "questions_1 = pd.read_csv('../input/question-csv-riiid/question_metadata.csv', usecols = columnas, dtype = dtypes)\n",
    "\n",
    "train = train.merge(questions_1, on = 'content_id', how='left')\n",
    "\n",
    "print('ACTUALIZACION DE FEATURES INICIADO')\n",
    "actualizar_features_inicio(train, features_dicts)\n",
    "print('ACTUALIZACION DE FEATURES FINALIZADO')\n",
    "\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T22:14:41.547604Z",
     "iopub.status.busy": "2020-12-31T22:14:41.532525Z",
     "iopub.status.idle": "2020-12-31T22:14:41.550862Z",
     "shell.execute_reply": "2020-12-31T22:14:41.550265Z"
    },
    "papermill": {
     "duration": 3.465164,
     "end_time": "2020-12-31T22:14:41.550990",
     "exception": false,
     "start_time": "2020-12-31T22:14:38.085826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(TARGET, FEATURES, model, questions, prior_question_elapsed_time_mean, features_dicts):\n",
    "    \n",
    "    # Get api iterator and predictor\n",
    "    env = riiideducation.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    set_predict = env.predict\n",
    "    \n",
    "    previous_test_df = None\n",
    "    for (test_df, sample_prediction_df) in iter_test:\n",
    "        if previous_test_df is not None:\n",
    "            previous_test_df[TARGET] = eval(test_df[\"prior_group_answers_correct\"].iloc[0])\n",
    "            actualizar_features(previous_test_df, features_dicts)\n",
    "            \n",
    "            #####\n",
    "            prev_test_df['answered_correctly'] = eval(test_df['prior_group_answers_correct'].iloc[0])\n",
    "            prev_test_df = prev_test_df[prev_test_df.content_type_id == False]\n",
    "            \n",
    "            prev_group = prev_test_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (\n",
    "                r['content_id'].values,\n",
    "                r['answered_correctly'].values))\n",
    "            for prev_user_id in prev_group.index:\n",
    "                prev_group_content = prev_group[prev_user_id][0]\n",
    "                prev_group_ac = prev_group[prev_user_id][1]\n",
    "                if prev_user_id in group.index:\n",
    "                    group[prev_user_id] = (np.append(group[prev_user_id][0],prev_group_content), \n",
    "                                           np.append(group[prev_user_id][1],prev_group_ac))\n",
    "                else:\n",
    "                    group[prev_user_id] = (prev_group_content,prev_group_ac)\n",
    "                if len(group[prev_user_id][0])>MAX_SEQ:\n",
    "                    new_group_content = group[prev_user_id][0][-MAX_SEQ:]\n",
    "                    new_group_ac = group[prev_user_id][1][-MAX_SEQ:]\n",
    "                    group[prev_user_id] = (new_group_content,new_group_ac)\n",
    "            #####\n",
    "            \n",
    "        #####\n",
    "        prev_test_df = test_df.copy()\n",
    "        #####\n",
    "        \n",
    "        test_df['prior_question_had_explanation'] = test_df.prior_question_had_explanation.fillna(False).astype('int8')\n",
    "        test_df['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean, inplace = True)\n",
    "        \n",
    "        previous_test_df = test_df.copy()\n",
    "        \n",
    "        test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop = True)\n",
    "        test_df = test_df.merge(questions, on = 'content_id', how='left')\n",
    "        test_df[TARGET] = 0\n",
    "        test_df = añadir_features(test_df, features_dicts)\n",
    "        \n",
    "        #####\n",
    "        test_dataset = TestDataset(group, test_df, skills)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=51200, shuffle=False)\n",
    "\n",
    "        SAKT_outs = []\n",
    "\n",
    "        for item in test_dataloader:\n",
    "            x = item[0].to(device).long()\n",
    "            target_id = item[1].to(device).long()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output, att_weight = SAKT_model(x, target_id)\n",
    "\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output[:, -1]\n",
    "            SAKT_outs.extend(output.view(-1).data.cpu().numpy())\n",
    "        #####\n",
    "        \n",
    "        test_df[TARGET] = np.array(SAKT_outs) * 0.5 + model.predict(test_df[FEATURES]) * 0.5 #media aritmetica (0.783)\n",
    "        #####\n",
    "        \n",
    "        #test_df[TARGET] =  model.predict(test_df[FEATURES])\n",
    "        set_predict(test_df[['row_id', TARGET]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T22:14:48.328678Z",
     "iopub.status.busy": "2020-12-31T22:14:48.327988Z",
     "iopub.status.idle": "2020-12-31T22:14:50.758293Z",
     "shell.execute_reply": "2020-12-31T22:14:50.757640Z"
    },
    "papermill": {
     "duration": 5.852212,
     "end_time": "2020-12-31T22:14:50.758413",
     "exception": false,
     "start_time": "2020-12-31T22:14:44.906201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Carga de questions\n",
    "columnas = ['question_id', 'bundle_id']\n",
    "dtypes={'question_id': 'uint16', 'bundle_id': 'uint8'}\n",
    "questions = pd.read_csv('../input/riiid-test-answer-prediction/questions.csv', usecols = columnas, dtype = dtypes)\n",
    "\n",
    "questions = questions.merge(questions_1, left_on = 'question_id', right_on = 'content_id', how = 'left')\n",
    "questions.drop(columns=['question_id'],  inplace = True)\n",
    "\n",
    "del questions_1, columnas, dtypes\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T22:14:57.483507Z",
     "iopub.status.busy": "2020-12-31T22:14:57.482697Z",
     "iopub.status.idle": "2020-12-31T22:14:58.059708Z",
     "shell.execute_reply": "2020-12-31T22:14:58.058999Z"
    },
    "papermill": {
     "duration": 3.965079,
     "end_time": "2020-12-31T22:14:58.059862",
     "exception": false,
     "start_time": "2020-12-31T22:14:54.094783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = lgb.Booster(model_file = '../input/lgbm-model-riiid-training/model_1.txt')\n",
    "TARGET = load_obj('TARGET')\n",
    "FEATURES = load_obj('FEATURES')\n",
    "prior_question_elapsed_time_mean = load_obj('prior_question_elapsed_time_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-31T22:15:04.770807Z",
     "iopub.status.busy": "2020-12-31T22:15:04.770134Z",
     "iopub.status.idle": "2020-12-31T22:15:06.526117Z",
     "shell.execute_reply": "2020-12-31T22:15:06.526599Z"
    },
    "papermill": {
     "duration": 5.12799,
     "end_time": "2020-12-31T22:15:06.526750",
     "exception": false,
     "start_time": "2020-12-31T22:15:01.398760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 13159.75it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 22536.56it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 24613.39it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 48354.49it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 19918.16it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 31923.86it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 21241.87it/s]\n"
     ]
    }
   ],
   "source": [
    "inference(TARGET, FEATURES, model, questions, prior_question_elapsed_time_mean, features_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1557.944057,
   "end_time": "2020-12-31T22:15:10.265839",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-31T21:49:12.321782",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
